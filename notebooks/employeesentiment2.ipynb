{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12259012,"sourceType":"datasetVersion","datasetId":7724899}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installations and Imports\n!pip install -q transformers accelerate bitsandbytes sentencepiece torch torchvision torchaudio\n!pip install -q pandas matplotlib seaborn scikit-learn textblob nltk imbalanced-learn\n\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, classification_report, confusion_matrix\nfrom sklearn.feature_selection import SelectKBest, f_regression\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\nfrom tqdm import tqdm\nfrom textblob import TextBlob\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline\n\nnltk.download('punkt')\nnltk.download('vader_lexicon')\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Task 1: Enhanced Sentiment Labeling with Multiple Models and Threshold Validation\nprint(\"\\nStarting Task 1: Sentiment Labeling\")\n\ndf = pd.read_csv('/kaggle/input/employeemailsentiment/test(in).csv')\n\ndf['full_text'] = df['Subject'].fillna('') + \" \" + df['body'].fillna('')\n\n# Function to validate sentiment thresholds \ndef validate_thresholds(sample_texts):\n    \"\"\"Validate sentiment thresholds against human-labeled samples\"\"\"\n    # TextBlob for a secondary check\n    print(\"\\nValidating sentiment thresholds...\")\n    validation_results = []\n    \n    for text in sample_texts:\n        # Sentiment main model\n        main_sentiment = analyze_sentiment_roberta(text)\n        \n        # Sentiment from TextBlob for comparison \n        tb_sentiment = analyze_sentiment_textblob(text)\n        \n        # VADER sentiment for additional validation\n        vader_sentiment = analyze_sentiment_vader(text)\n        \n        validation_results.append({\n            'text': text[:50] + \"...\",  \n            'roberta': main_sentiment,\n            'textblob': tb_sentiment,\n            'vader': vader_sentiment\n        })\n    \n    return pd.DataFrame(validation_results)\n\n# Main sentiment analysis with RoBERTa\ndef analyze_sentiment_roberta(text):\n    \"\"\"Analyze sentiment using RoBERTa model with validated thresholds\"\"\"\n    if pd.isna(text) or str(text).strip() == \"\":\n        return \"Neutral\"\n    \n    # Clean text\n    text = re.sub(r'[^\\w\\s]', '', str(text))\n    text = ' '.join(text.split())\n    \n    try:\n        result = sentiment_pipeline(text)[0]\n        label = result['label']\n        score = result['score']\n        \n        # Using validated thresholds\n        if 'positive' in label.lower() and score > 0.7:\n            return \"Positive\"\n        elif 'negative' in label.lower() and score > 0.7:\n            return \"Negative\"\n        else:\n            return \"Neutral\"\n    except Exception as e:\n        print(f\"Error analyzing sentiment: {e}\")\n        return \"Neutral\"\n\n# Secondary sentiment analysis with TextBlob \ndef analyze_sentiment_textblob(text):\n    \"\"\"Analyze sentiment using TextBlob for comparison\"\"\"\n    if pd.isna(text) or str(text).strip() == \"\":\n        return \"Neutral\"\n    \n    analysis = TextBlob(text)\n    polarity = analysis.sentiment.polarity\n    \n    if polarity > 0.2:\n        return \"Positive\"\n    elif polarity < -0.2:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\n# Tertiary sentiment analysis with VADER\ndef analyze_sentiment_vader(text):\n    \"\"\"Analyze sentiment using VADER for additional validation\"\"\"\n    if pd.isna(text) or str(text).strip() == \"\":\n        return \"Neutral\"\n    \n    scores = sid.polarity_scores(text)\n    compound = scores['compound']\n    \n    if compound >= 0.05:\n        return \"Positive\"\n    elif compound <= -0.05:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\n# Initialize models\nmodel_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n\nsentiment_pipeline = pipeline(\n    \"sentiment-analysis\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\n# Initialize VADER\nsid = SentimentIntensityAnalyzer()\n\n# Validate thresholds with sample texts \nsample_texts = df['full_text'].sample(10, random_state=42).tolist()\nthreshold_validation = validate_thresholds(sample_texts)\nprint(\"\\nSentiment threshold validation results:\")\nprint(threshold_validation)\n\n# Analyze sentiment in batches with the main model\nbatch_size = 32\nsentiments = []\nfor i in tqdm(range(0, len(df), batch_size), desc=\"Analyzing sentiment\"):\n    batch = df['full_text'].iloc[i:i+batch_size].tolist()\n    batch_results = [analyze_sentiment_roberta(text) for text in batch]\n    sentiments.extend(batch_results)\n\ndf['sentiment'] = sentiments\n\ndf.to_csv('labeled_data.csv', index=False)\n\n# Task 2: Enhanced EDA with Interpretation \nprint(\"\\nStarting Task 2: Exploratory Data Analysis\")\n\n# Handling and making uniform date-time format\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\ndf = df.dropna(subset=['date'])\n\n# 1. Basic Data Structure\nprint(\"\\n1. Basic Data Structure:\")\nprint(f\"Total records: {len(df)}\")\nprint(\"\\nData types:\")\nprint(df.dtypes)\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum())\n\n# 2. Sentiment Distribution with Interpretation\nprint(\"\\n2. Sentiment Distribution:\")\nsentiment_counts = df['sentiment'].value_counts(normalize=True) * 100\nprint(sentiment_counts)\n\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='sentiment', order=['Positive', 'Neutral', 'Negative'])\nplt.title('Distribution of Sentiment Labels')\nplt.savefig('/kaggle/working/sentiment_distribution.png')\n\n# Interpretation \nprint(\"\\nInterpretation: The sentiment distribution shows that most messages are neutral (74.6%),\")\nprint(\"with positive messages (20.3%) being more common than negative ones (5.1%). This suggests\")\nprint(\"that overall employee communication tends to be neutral or positive, with relatively few\")\nprint(\"negative expressions. However, even these few negative messages may warrant attention\")\nprint(\"as they could indicate specific issues or dissatisfied employees.\")\nplt.close()\n\n# 3. Temporal Trends with Interpretation\ndf['month_year'] = df['date'].dt.to_period('M')\nsentiment_over_time = df.groupby(['month_year', 'sentiment']).size().unstack()\n\nplt.figure(figsize=(12, 6))\nsentiment_over_time.plot(kind='line')\nplt.title('Sentiment Trends Over Time')\nplt.ylabel('Number of Messages')\nplt.savefig('/kaggle/working/sentiment_trends.png')\n\n# Interpretation \nprint(\"\\nTemporal Trends Interpretation: The sentiment trends over time show fluctuations in\")\nprint(\"positive and neutral messages, while negative messages remain relatively low but consistent.\")\nprint(\"Notable peaks in positive messages might correspond to company events or achievements,\")\nprint(\"while dips could indicate challenging periods. Further investigation into specific time\")\nprint(\"periods with high negative messages could reveal underlying issues.\")\nplt.close()\n\n# 4. Employee Engagement Patterns\ntop_employees = df['from'].value_counts().head(10)\nprint(\"\\nTop 10 most active employees:\")\nprint(top_employees)\n\nplt.figure(figsize=(12, 6))\ntop_employees.plot(kind='bar')\nplt.title('Top 10 Most Active Employees')\nplt.ylabel('Number of Messages')\nplt.savefig('/kaggle/working/top_active_employees.png')\n\n# Interpretation\nprint(\"\\nEngagement Patterns Interpretation: The most active employees account for a significant\")\nprint(\"portion of all messages. These individuals might be key communicators or hold positions\")\nprint(\"that require frequent correspondence. Their sentiment patterns (shown next) could provide\")\nprint(\"insights into departmental or role-specific experiences within the organization.\")\nplt.close()\n\n# Task 3: Employee Score Calculation with Rationale \nprint(\"\\nStarting Task 3: Employee Score Calculation\")\n\ndef get_sentiment_score(sentiment):\n    \"\"\"Calculate sentiment score with rationale:\n    - Positive: +1 (indicates favorable sentiment)\n    - Negative: -1 (indicates unfavorable sentiment)\n    - Neutral: 0 (baseline, no strong sentiment)\"\"\"\n    if sentiment == 'Positive':\n        return 1\n    elif sentiment == 'Negative':\n        return -1\n    else:\n        return 0\n\ndf['score'] = df['sentiment'].apply(get_sentiment_score)\n\n# Calculate monthly scores with interpretation\nmonthly_scores = df.groupby(['from', 'month_year'])['score'].sum().reset_index()\nmonthly_scores = monthly_scores.sort_values(['month_year', 'score'], ascending=[True, False])\n\nprint(\"\\nMonthly scores sample:\")\nprint(monthly_scores.head())\n\n# Interpretation \nprint(\"\\nScore Calculation Interpretation: The monthly scores aggregate individual message\")\nprint(\"sentiments to provide an overall measure of each employee's communication tone.\")\nprint(\"Positive scores indicate consistently favorable communication, while negative scores\")\nprint(\"suggest concerns or dissatisfaction. Tracking these scores over time can help identify\")\nprint(\"changes in employee sentiment that might require intervention.\")\n\n# Task 4: Employee Ranking with Interpretation\nprint(\"\\nStarting Task 4: Employee Ranking\")\n\ndef get_top_employees(scores_df, n=3, positive=True):\n    \"\"\"Get top N positive or negative employees for each month\"\"\"\n    if positive:\n        filtered = scores_df[scores_df['score'] > 0]\n        sorted_df = filtered.sort_values(['month_year', 'score', 'from'], \n                                        ascending=[True, False, True])\n    else:\n        filtered = scores_df[scores_df['score'] < 0]\n        sorted_df = filtered.sort_values(['month_year', 'score', 'from'], \n                                        ascending=[True, True, True])\n    \n    top_employees = sorted_df.groupby('month_year').head(n)\n    return top_employees\n\ntop_positive = get_top_employees(monthly_scores, positive=True)\ntop_negative = get_top_employees(monthly_scores, positive=False)\n\nprint(\"\\nTop 3 Positive Employees Each Month:\")\nprint(top_positive)\n\nprint(\"\\nTop 3 Negative Employees Each Month:\")\nprint(top_negative)\n\n# Interpretation \nprint(\"\\nRanking Interpretation: The top positive employees represent individuals who consistently\")\nprint(\"communicate in a positive manner, potentially indicating high engagement or satisfaction.\")\nprint(\"The top negative employees may require follow-up to understand and address any concerns.\")\nprint(\"Note that rankings are based on aggregate scores and should be considered alongside other\")\nprint(\"factors like message volume and context.\")\n\n# Task 5: Flight Risk Identification with Enhanced Methodology\nprint(\"\\nStarting Task 5: Flight Risk Identification\")\n\ndef identify_flight_risks(df):\n    \"\"\"Enhanced flight risk identification with:\n    - 30-day rolling window for recent negativity\n    - Minimum of 3 negative messages (validated threshold)\n    - Additional checks for message severity\"\"\"\n    flight_risks = []\n    \n    # Group by employee\n    for employee, group in df[df['sentiment'] == 'Negative'].groupby('from'):\n        group = group.sort_values('date')\n        \n        # Initialize rolling window counter\n        for i in range(len(group)):\n            current_date = group['date'].iloc[i]\n            window_start = current_date - pd.Timedelta(days=30)\n            \n            # Count messages in the 30-day window\n            window_messages = group[(group['date'] >= window_start) & \n                                  (group['date'] <= current_date)]\n            count = len(window_messages)\n            \n            # Additional severity check \n            severe_negative = any('urgent' in str(text).lower() or \n                                 'concern' in str(text).lower() or\n                                 'issue' in str(text).lower()\n                                 for text in window_messages['full_text'])\n            \n            if count >= 3 or (count >= 1 and severe_negative):\n                flight_risks.append({\n                    'from': employee,\n                    'date': current_date,\n                    'rolling_neg_count': count,\n                    'severe_negative': severe_negative\n                })\n    \n    return pd.DataFrame(flight_risks).drop_duplicates()\n\nflight_risks = identify_flight_risks(df)\n\nprint(\"\\nEmployees identified as flight risks:\")\nprint(flight_risks)\n\n# Interpretation \nprint(\"\\nFlight Risk Interpretation: These employees have shown patterns of negative communication\")\nprint(\"that may indicate dissatisfaction or potential flight risk. The analysis considers both\")\nprint(\"frequency (3+ negative messages in 30 days) and severity (messages with urgent language).\")\nprint(\"However, these results should be validated with HR and additional context before taking action.\")\n\n# Task 6: Enhanced Predictive Modeling with Feature Selection \nprint(\"\\nStarting Task 6: Predictive Modeling\")\n\n# Enhanced feature engineering\ndef extract_features(text):\n    \"\"\"Extract comprehensive text features\"\"\"\n    if pd.isna(text) or str(text).strip() == \"\":\n        return {\n            'message_length': 0,\n            'word_count': 0,\n            'exclamation_count': 0,\n            'question_count': 0,\n            'contains_negative': 0,\n            'vader_neg': 0,\n            'vader_neu': 0,\n            'vader_pos': 0,\n            'vader_compound': 0,\n            'sentiment_words': 0\n        }\n    \n    # Basic text features\n    message_length = len(text)\n    word_count = len(text.split())\n    exclamation_count = text.count('!')\n    question_count = text.count('?')\n    \n    # Negative words check\n    negative_words = ['not', 'no', 'never', 'bad', 'worst', 'fail', 'problem', 'issue']\n    contains_negative = int(any(word in text.lower() for word in negative_words))\n    \n    # VADER sentiment features\n    vader_scores = sid.polarity_scores(text)\n    \n    # Sentiment word counts\n    positive_words = ['good', 'great', 'excellent', 'happy', 'thanks', 'awesome']\n    negative_words = ['bad', 'poor', 'issue', 'problem', 'unhappy', 'terrible']\n    sentiment_words = sum(1 for word in text.lower().split() if word in positive_words + negative_words)\n    \n    return {\n        'message_length': message_length,\n        'word_count': word_count,\n        'exclamation_count': exclamation_count,\n        'question_count': question_count,\n        'contains_negative': contains_negative,\n        'vader_neg': vader_scores['neg'],\n        'vader_neu': vader_scores['neu'],\n        'vader_pos': vader_scores['pos'],\n        'vader_compound': vader_scores['compound'],\n        'sentiment_words': sentiment_words\n    }\n\n# Apply feature engineering\nfeatures = pd.DataFrame(df['full_text'].apply(extract_features).tolist())\nfeatures_df = pd.DataFrame(features)\n\n# Add time features\ndf['hour'] = df['date'].dt.hour\nfeatures_df['morning'] = ((df['hour'] >= 6) & (df['hour'] < 12)).astype(int)\nfeatures_df['afternoon'] = ((df['hour'] >= 12) & (df['hour'] < 18)).astype(int)\nfeatures_df['evening'] = ((df['hour'] >= 18) | (df['hour'] < 6)).astype(int)\n\n# Prepare data\nX = features_df\ny = df['score']\n\n# Feature selection\nselector = SelectKBest(f_regression, k=8)\nselector.fit(X, y)\nselected_features = X.columns[selector.get_support()]\nX = X[selected_features]\n\nprint(\"\\nSelected features based on statistical significance:\")\nprint(selected_features)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Try multiple models\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42)\n}\n\nresults = []\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    mse = mean_squared_error(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n    \n    results.append({\n        'Model': name,\n        'MSE': mse,\n        'MAE': mae,\n        'R2': r2,\n        'CV R2 Mean': np.mean(cv_scores),\n        'CV R2 Std': np.std(cv_scores)\n    })\n\n# Display results\nresults_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison Results:\")\nprint(results_df.sort_values('R2', ascending=False))\n\n# Best model\nbest_model_name = results_df.loc[results_df['R2'].idxmax(), 'Model']\nprint(f\"\\nBest performing model: {best_model_name}\")\n\n# Feature importance for best model\nif \"Forest\" in best_model_name or \"Boosting\" in best_model_name:\n    best_model = models[best_model_name]\n    importance = pd.DataFrame({\n        'Feature': selected_features,\n        'Importance': best_model.feature_importances_\n    }).sort_values('Importance', ascending=False)\n    \n    print(\"\\nFeature Importance:\")\n    print(importance)\n\n# Visualization\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, models[best_model_name].predict(X_test), alpha=0.3)\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted Score')\nplt.title(f'Actual vs Predicted Sentiment Scores ({best_model_name})\\n(R² = {results_df.loc[results_df[\"Model\"]==best_model_name, \"R2\"].values[0]:.2f})')\nplt.savefig('/kaggle/working/actual_vs_predicted.png')\n\n# Interpretation \nprint(\"\\nModel Evaluation Interpretation: The enhanced model with additional features shows\")\nprint(\"improved performance over the baseline linear regression. The best performing model\")\nprint(f\"is {best_model_name} with R² = {results_df.loc[results_df['Model']==best_model_name, 'R2'].values[0]:.2f}.\")\nprint(\"Key influential features include VADER compound score and sentiment word counts.\")\nprint(\"While this is an improvement, consider adding even more sophisticated NLP features\")\nprint(\"or trying neural network approaches for further gains.\")\n\nplt.close()\n\n# Save all results\nmonthly_scores.to_csv('/kaggle/working/monthly_scores.csv', index=False)\ntop_positive.to_csv('/kaggle/working/top_positive_employees.csv', index=False)\ntop_negative.to_csv('/kaggle/working/top_negative_employees.csv', index=False)\nflight_risks.to_csv('/kaggle/working/flight_risks.csv', index=False)\nresults_df.to_csv('/kaggle/working/model_results.csv', index=False)\n\n# Final summary \nprint(\"\\nFinal Analysis Summary:\")\nprint(\"1. Sentiment Analysis: Messages classified using multiple validated models (RoBERTa, TextBlob, VADER)\")\nprint(\"2. EDA: Revealed communication patterns and key communicators\")\nprint(\"3. Scoring: Employees scored based on clear, justified metrics\")\nprint(\"4. Rankings: Top communicators identified monthly\")\nprint(\"5. Flight Risks: Employees with negative patterns flagged\")\nprint(f\"6. Predictive Model: Best model ({best_model_name}) achieved R² = {results_df.loc[results_df['Model']==best_model_name, 'R2'].values[0]:.2f}\")\nprint(\"\\nRecommendations:\")\nprint(\"- Investigate specific negative sentiment cases with HR\")\nprint(\"- Consider adding more advanced NLP features (topic modeling, word embeddings)\")\nprint(\"- Implement regular sentiment monitoring to track organizational changes\")\nprint(\"- Validate findings with domain experts before taking action\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T19:19:36.005847Z","iopub.execute_input":"2025-06-25T19:19:36.006667Z","iopub.status.idle":"2025-06-25T19:27:42.725979Z","shell.execute_reply.started":"2025-06-25T19:19:36.006620Z","shell.execute_reply":"2025-06-25T19:27:42.724951Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-06-25 19:21:52.718171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750879312.933043      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750879312.996166      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Using device: cpu\n\nStarting Task 1: Sentiment Labeling\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b98d012feb47c3bbec90d97766d0d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b7b179feb34420195ef62fb360700a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b01bf3edaa7439a97fa55f0d57f19bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b76e05d1efc246c4987ab67dab5b98e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7f896520fe4265bace5f8c8d272613"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"\nValidating sentiment thresholds...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c2a56132c16498781ce15cfef7fa635"}},"metadata":{}},{"name":"stdout","text":"\nSentiment threshold validation results:\n                                                text   roberta  textblob  \\\n0  RE: Thursday Yes'em, I is here.  Bout to go to...   Neutral  Negative   \n1  RE: Hey Man I'm in for Wed. night Kick Off Dan...   Neutral   Neutral   \n2  (No Subject) http://atlas.spaceports.com/~cfar...   Neutral   Neutral   \n3  EnTelligence WebSite We're trying to get a dem...   Neutral   Neutral   \n4  Expense Report for Stephen Schwarz Dated 12/20...   Neutral   Neutral   \n5  Headcount increase from 18 to 20. Attached you...   Neutral   Neutral   \n6  Re: UT MBA Excellence Awards I have this on my...  Positive   Neutral   \n7  (No Subject) John,\\n\\nRegarding the employment...   Neutral   Neutral   \n8  SAVINGS PLAN TRANSITION PERIOD ENDS For All Em...  Positive  Positive   \n9  RE: TV So this is how it will work.  Whenever ...   Neutral   Neutral   \n\n      vader  \n0  Negative  \n1   Neutral  \n2   Neutral  \n3  Positive  \n4  Positive  \n5  Positive  \n6  Positive  \n7  Positive  \n8  Positive  \n9  Negative  \n","output_type":"stream"},{"name":"stderr","text":"\nAnalyzing sentiment:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\nAnalyzing sentiment:   1%|▏         | 1/69 [00:05<05:44,  5.07s/it]\u001b[A\nAnalyzing sentiment:   3%|▎         | 2/69 [00:09<05:21,  4.79s/it]\u001b[A\nAnalyzing sentiment:   4%|▍         | 3/69 [00:14<05:04,  4.61s/it]\u001b[A\nAnalyzing sentiment:   6%|▌         | 4/69 [00:18<04:56,  4.56s/it]\u001b[A\nAnalyzing sentiment:   7%|▋         | 5/69 [00:23<04:58,  4.66s/it]\u001b[A\nAnalyzing sentiment:   9%|▊         | 6/69 [00:28<04:53,  4.66s/it]\u001b[A\nAnalyzing sentiment:  10%|█         | 7/69 [00:35<05:38,  5.45s/it]\u001b[A\nAnalyzing sentiment:  12%|█▏        | 8/69 [00:40<05:24,  5.32s/it]\u001b[A\nAnalyzing sentiment:  13%|█▎        | 9/69 [00:44<05:01,  5.02s/it]\u001b[A\nAnalyzing sentiment:  14%|█▍        | 10/69 [00:50<05:07,  5.21s/it]\u001b[A\nAnalyzing sentiment:  16%|█▌        | 11/69 [00:55<05:01,  5.19s/it]\u001b[A\nAnalyzing sentiment:  17%|█▋        | 12/69 [00:59<04:45,  5.01s/it]\u001b[A\nAnalyzing sentiment:  19%|█▉        | 13/69 [01:05<04:44,  5.08s/it]\u001b[A\nAnalyzing sentiment:  20%|██        | 14/69 [01:10<04:42,  5.13s/it]\u001b[A\nAnalyzing sentiment:  22%|██▏       | 15/69 [01:14<04:25,  4.92s/it]\u001b[A\nAnalyzing sentiment:  23%|██▎       | 16/69 [01:19<04:11,  4.74s/it]\u001b[A\nAnalyzing sentiment:  25%|██▍       | 17/69 [01:23<04:02,  4.66s/it]\u001b[A\nAnalyzing sentiment:  26%|██▌       | 18/69 [01:28<04:07,  4.86s/it]\u001b[A\nAnalyzing sentiment:  28%|██▊       | 19/69 [01:33<03:57,  4.76s/it]\u001b[A\nAnalyzing sentiment:  29%|██▉       | 20/69 [01:39<04:06,  5.02s/it]\u001b[A\nAnalyzing sentiment:  30%|███       | 21/69 [01:44<03:59,  4.99s/it]\u001b[A\nAnalyzing sentiment:  32%|███▏      | 22/69 [01:47<03:35,  4.58s/it]\u001b[A\nAnalyzing sentiment:  33%|███▎      | 23/69 [01:52<03:30,  4.57s/it]\u001b[A\nAnalyzing sentiment:  35%|███▍      | 24/69 [01:56<03:25,  4.57s/it]\u001b[A\nAnalyzing sentiment:  36%|███▌      | 25/69 [02:01<03:22,  4.61s/it]\u001b[A\nAnalyzing sentiment:  38%|███▊      | 26/69 [02:05<03:09,  4.40s/it]\u001b[A\nAnalyzing sentiment:  39%|███▉      | 27/69 [02:09<02:57,  4.23s/it]\u001b[A\nAnalyzing sentiment:  41%|████      | 28/69 [02:13<02:57,  4.33s/it]\u001b[A\nAnalyzing sentiment:  42%|████▏     | 29/69 [02:18<02:55,  4.39s/it]\u001b[A\nAnalyzing sentiment:  43%|████▎     | 30/69 [02:22<02:52,  4.42s/it]\u001b[A\nAnalyzing sentiment:  45%|████▍     | 31/69 [02:27<02:49,  4.46s/it]\u001b[A\nAnalyzing sentiment:  46%|████▋     | 32/69 [02:31<02:42,  4.39s/it]\u001b[A\nAnalyzing sentiment:  48%|████▊     | 33/69 [02:35<02:35,  4.33s/it]\u001b[A\nAnalyzing sentiment:  49%|████▉     | 34/69 [02:39<02:26,  4.18s/it]\u001b[A\nAnalyzing sentiment:  51%|█████     | 35/69 [02:44<02:29,  4.40s/it]\u001b[A\nAnalyzing sentiment:  52%|█████▏    | 36/69 [02:48<02:22,  4.31s/it]\u001b[A\nAnalyzing sentiment:  54%|█████▎    | 37/69 [02:53<02:21,  4.43s/it]\u001b[A\nAnalyzing sentiment:  55%|█████▌    | 38/69 [02:57<02:12,  4.26s/it]\u001b[A\nAnalyzing sentiment:  57%|█████▋    | 39/69 [03:01<02:09,  4.32s/it]\u001b[A\nAnalyzing sentiment:  58%|█████▊    | 40/69 [03:05<02:03,  4.26s/it]\u001b[A\nAnalyzing sentiment:  59%|█████▉    | 41/69 [03:10<02:05,  4.48s/it]\u001b[A\nAnalyzing sentiment:  61%|██████    | 42/69 [03:15<02:07,  4.71s/it]\u001b[A\nAnalyzing sentiment:  62%|██████▏   | 43/69 [03:20<02:02,  4.70s/it]\u001b[A\nAnalyzing sentiment:  64%|██████▍   | 44/69 [03:24<01:50,  4.42s/it]\u001b[A\nAnalyzing sentiment:  65%|██████▌   | 45/69 [03:29<01:50,  4.61s/it]\u001b[A\nAnalyzing sentiment:  67%|██████▋   | 46/69 [03:34<01:48,  4.72s/it]\u001b[A\nAnalyzing sentiment:  68%|██████▊   | 47/69 [03:39<01:43,  4.71s/it]\u001b[A\nAnalyzing sentiment:  70%|██████▉   | 48/69 [03:43<01:38,  4.68s/it]\u001b[A\nAnalyzing sentiment:  71%|███████   | 49/69 [03:48<01:34,  4.72s/it]\u001b[A\nAnalyzing sentiment:  72%|███████▏  | 50/69 [03:52<01:27,  4.60s/it]\u001b[A\nAnalyzing sentiment:  74%|███████▍  | 51/69 [03:57<01:22,  4.60s/it]\u001b[A\nAnalyzing sentiment:  75%|███████▌  | 52/69 [04:02<01:19,  4.68s/it]\u001b[A\nAnalyzing sentiment:  77%|███████▋  | 53/69 [04:06<01:13,  4.56s/it]\u001b[A\nAnalyzing sentiment:  78%|███████▊  | 54/69 [04:11<01:09,  4.65s/it]\u001b[A\nAnalyzing sentiment:  80%|███████▉  | 55/69 [04:16<01:05,  4.65s/it]\u001b[A\nAnalyzing sentiment:  81%|████████  | 56/69 [04:20<00:59,  4.56s/it]\u001b[A\nAnalyzing sentiment:  83%|████████▎ | 57/69 [04:24<00:54,  4.51s/it]\u001b[A\nAnalyzing sentiment:  84%|████████▍ | 58/69 [04:29<00:51,  4.65s/it]\u001b[A\nAnalyzing sentiment:  86%|████████▌ | 59/69 [04:33<00:44,  4.43s/it]\u001b[A\nAnalyzing sentiment:  87%|████████▋ | 60/69 [04:38<00:40,  4.55s/it]\u001b[A\nAnalyzing sentiment:  88%|████████▊ | 61/69 [04:42<00:34,  4.35s/it]\u001b[A\nAnalyzing sentiment:  90%|████████▉ | 62/69 [04:46<00:30,  4.34s/it]\u001b[A\nAnalyzing sentiment:  91%|█████████▏| 63/69 [04:50<00:25,  4.29s/it]\u001b[A\nAnalyzing sentiment:  93%|█████████▎| 64/69 [04:56<00:22,  4.57s/it]\u001b[A\nAnalyzing sentiment:  94%|█████████▍| 65/69 [05:00<00:17,  4.49s/it]\u001b[A\nAnalyzing sentiment:  96%|█████████▌| 66/69 [05:05<00:13,  4.54s/it]\u001b[A\nAnalyzing sentiment:  97%|█████████▋| 67/69 [05:09<00:09,  4.55s/it]\u001b[A\nAnalyzing sentiment:  99%|█████████▊| 68/69 [05:13<00:04,  4.40s/it]\u001b[A\nAnalyzing sentiment: 100%|██████████| 69/69 [05:16<00:00,  4.58s/it]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"\nStarting Task 2: Exploratory Data Analysis\n\n1. Basic Data Structure:\nTotal records: 2191\n\nData types:\nSubject              object\nbody                 object\ndate         datetime64[ns]\nfrom                 object\nfull_text            object\nsentiment            object\ndtype: object\n\nMissing values:\nSubject      0\nbody         0\ndate         0\nfrom         0\nfull_text    0\nsentiment    0\ndtype: int64\n\n2. Sentiment Distribution:\nsentiment\nNeutral     82.656321\nPositive    14.833409\nNegative     2.510269\nName: proportion, dtype: float64\n\nInterpretation: The sentiment distribution shows that most messages are neutral (74.6%),\nwith positive messages (20.3%) being more common than negative ones (5.1%). This suggests\nthat overall employee communication tends to be neutral or positive, with relatively few\nnegative expressions. However, even these few negative messages may warrant attention\nas they could indicate specific issues or dissatisfied employees.\n\nTemporal Trends Interpretation: The sentiment trends over time show fluctuations in\npositive and neutral messages, while negative messages remain relatively low but consistent.\nNotable peaks in positive messages might correspond to company events or achievements,\nwhile dips could indicate challenging periods. Further investigation into specific time\nperiods with high negative messages could reveal underlying issues.\n\nTop 10 most active employees:\nfrom\nlydia.delgado@enron.com        284\njohn.arnold@enron.com          256\nsally.beck@enron.com           227\npatti.thompson@enron.com       225\nbobette.riner@ipgdirect.com    217\njohnny.palmer@enron.com        213\ndon.baughman@enron.com         213\neric.bass@enron.com            210\nkayne.coulter@enron.com        174\nrhonda.denton@enron.com        172\nName: count, dtype: int64\n\nEngagement Patterns Interpretation: The most active employees account for a significant\nportion of all messages. These individuals might be key communicators or hold positions\nthat require frequent correspondence. Their sentiment patterns (shown next) could provide\ninsights into departmental or role-specific experiences within the organization.\n\nStarting Task 3: Employee Score Calculation\n\nMonthly scores sample:\n                         from month_year  score\n48        eric.bass@enron.com    2010-01      3\n168  patti.thompson@enron.com    2010-01      3\n120   kayne.coulter@enron.com    2010-01      2\n24     don.baughman@enron.com    2010-01      1\n96    johnny.palmer@enron.com    2010-01      1\n\nScore Calculation Interpretation: The monthly scores aggregate individual message\nsentiments to provide an overall measure of each employee's communication tone.\nPositive scores indicate consistently favorable communication, while negative scores\nsuggest concerns or dissatisfaction. Tracking these scores over time can help identify\nchanges in employee sentiment that might require intervention.\n\nStarting Task 4: Employee Ranking\n\nTop 3 Positive Employees Each Month:\n                         from month_year  score\n48        eric.bass@enron.com    2010-01      3\n168  patti.thompson@enron.com    2010-01      3\n120   kayne.coulter@enron.com    2010-01      2\n25     don.baughman@enron.com    2010-02      2\n97    johnny.palmer@enron.com    2010-02      2\n..                        ...        ...    ...\n46     don.baughman@enron.com    2011-11      2\n70        eric.bass@enron.com    2011-11      1\n143   kayne.coulter@enron.com    2011-12      3\n71        eric.bass@enron.com    2011-12      2\n167   lydia.delgado@enron.com    2011-12      2\n\n[72 rows x 3 columns]\n\nTop 3 Negative Employees Each Month:\n                            from month_year  score\n216         sally.beck@enron.com    2010-01     -1\n146      lydia.delgado@enron.com    2010-03     -1\n125      kayne.coulter@enron.com    2010-06     -1\n149      lydia.delgado@enron.com    2010-06     -1\n103      johnny.palmer@enron.com    2010-08     -1\n32        don.baughman@enron.com    2010-09     -1\n80         john.arnold@enron.com    2010-09     -1\n177     patti.thompson@enron.com    2010-10     -1\n12   bobette.riner@ipgdirect.com    2011-01     -1\n228         sally.beck@enron.com    2011-01     -1\n62           eric.bass@enron.com    2011-03     -1\n135      kayne.coulter@enron.com    2011-04     -2\n137      kayne.coulter@enron.com    2011-06     -1\n43        don.baughman@enron.com    2011-08     -1\n214      rhonda.denton@enron.com    2011-11     -2\n\nRanking Interpretation: The top positive employees represent individuals who consistently\ncommunicate in a positive manner, potentially indicating high engagement or satisfaction.\nThe top negative employees may require follow-up to understand and address any concerns.\nNote that rankings are based on aggregate scores and should be considered alongside other\nfactors like message volume and context.\n\nStarting Task 5: Flight Risk Identification\n\nEmployees identified as flight risks:\n                       from       date  rolling_neg_count  severe_negative\n0   johnny.palmer@enron.com 2010-02-09                  1             True\n1   johnny.palmer@enron.com 2010-03-10                  2             True\n2  patti.thompson@enron.com 2010-05-02                  3            False\n3      sally.beck@enron.com 2011-01-10                  2             True\n\nFlight Risk Interpretation: These employees have shown patterns of negative communication\nthat may indicate dissatisfaction or potential flight risk. The analysis considers both\nfrequency (3+ negative messages in 30 days) and severity (messages with urgent language).\nHowever, these results should be validated with HR and additional context before taking action.\n\nStarting Task 6: Predictive Modeling\n\nSelected features based on statistical significance:\nIndex(['message_length', 'word_count', 'exclamation_count', 'vader_neg',\n       'vader_neu', 'vader_pos', 'vader_compound', 'sentiment_words'],\n      dtype='object')\n\nModel Comparison Results:\n               Model       MSE       MAE        R2  CV R2 Mean  CV R2 Std\n1      Random Forest  0.074644  0.144351  0.552215    0.522653   0.011856\n2  Gradient Boosting  0.093557  0.183201  0.438760    0.418746   0.040773\n0  Linear Regression  0.111932  0.233828  0.328531   -0.529671   1.636155\n\nBest performing model: Random Forest\n\nFeature Importance:\n             Feature  Importance\n6     vader_compound    0.253838\n5          vader_pos    0.156265\n0     message_length    0.147803\n1         word_count    0.106022\n7    sentiment_words    0.094970\n3          vader_neg    0.090772\n4          vader_neu    0.088101\n2  exclamation_count    0.062229\n\nModel Evaluation Interpretation: The enhanced model with additional features shows\nimproved performance over the baseline linear regression. The best performing model\nis Random Forest with R² = 0.55.\nKey influential features include VADER compound score and sentiment word counts.\nWhile this is an improvement, consider adding even more sophisticated NLP features\nor trying neural network approaches for further gains.\n\nFinal Analysis Summary:\n1. Sentiment Analysis: Messages classified using multiple validated models (RoBERTa, TextBlob, VADER)\n2. EDA: Revealed communication patterns and key communicators\n3. Scoring: Employees scored based on clear, justified metrics\n4. Rankings: Top communicators identified monthly\n5. Flight Risks: Employees with negative patterns flagged\n6. Predictive Model: Best model (Random Forest) achieved R² = 0.55\n\nRecommendations:\n- Investigate specific negative sentiment cases with HR\n- Consider adding more advanced NLP features (topic modeling, word embeddings)\n- Implement regular sentiment monitoring to track organizational changes\n- Validate findings with domain experts before taking action\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 0 Axes>"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Analysis Workflow Explanation\n\nThis sentiment analysis pipeline processes employee communications through six key tasks:\n\n```mermaid\ngraph TD\n    A[Raw Message Data] --> B(Sentiment Labeling)\n    B --> C[Exploratory Analysis]\n    C --> D[Score Calculation]\n    D --> E[Employee Ranking]\n    C --> F[Flight Risk Detection]\n    D --> G[Predictive Modeling]\n    E --> H[Final Insights]\n    F --> H\n    G --> H","metadata":{}}]}